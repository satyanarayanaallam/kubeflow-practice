{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/satyanarayanaallam/kubeflow-practice/blob/main/kf_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTUNBTUIoK0A"
      },
      "source": [
        "### Topics Covered\n",
        "\n",
        "**Kubeflow Pipeline Building Demo - IRIS classifier Model training and prediction**\n",
        "1. Python function needed to train and predict\n",
        "2. Creating components from python functions\n",
        "3. Initialise kubeflow pipeline\n",
        "4. define the pipeline function and put together all the components\n",
        "5. Mounting volume for component's output storage\n",
        "6. Compiling pipeline and generating yaml - it can be directly uploaded to kubeflow and create experiments and runs using UI\n",
        "7. Create run from pipeline function using the code\n",
        "8. How to disable cache to see the each steps output on second and successive runs\n",
        "\n",
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HAfF2EDeoK0C",
        "outputId": "8d2d3c9d-7db1-4736-be93-9c19a6c45d89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-5e161f88ee94>:2: DeprecationWarning: The module `kfp.v2` is deprecated and will be removed in a futureversion. Please import directly from the `kfp` namespace, instead of `kfp.v2`.\n",
            "  import kfp.v2.components as comp\n"
          ]
        }
      ],
      "source": [
        "import kfp\n",
        "import kfp.v2.components as comp\n",
        "import requests\n",
        "import kfp.dsl as dsl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ByKp1xlQoK0D",
        "outputId": "bf8e4418-d819-4157-e3b2-47c6f0c41147"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting kfp==1.8.18\n",
            "  Downloading kfp-1.8.18.tar.gz (304 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m304.8/304.8 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py<2,>=0.9 in /usr/local/lib/python3.10/dist-packages (from kfp==1.8.18) (1.4.0)\n",
            "Collecting PyYAML<6,>=5.3 (from kfp==1.8.18)\n",
            "  Downloading PyYAML-5.4.1.tar.gz (175 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.1/175.1 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
            "\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "Name: kfp\n",
            "Version: 2.6.0\n",
            "Summary: Kubeflow Pipelines SDK\n",
            "Home-page: https://github.com/kubeflow/pipelines\n",
            "Author: The Kubeflow Authors\n",
            "Author-email: \n",
            "License: \n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: click, docstring-parser, google-api-core, google-auth, google-cloud-storage, kfp-pipeline-spec, kfp-server-api, kubernetes, protobuf, PyYAML, requests-toolbelt, tabulate, urllib3\n",
            "Required-by: \n"
          ]
        }
      ],
      "source": [
        "!pip install kfp==1.8.18\n",
        "!pip show kfp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "nYTT_YpXoK0E"
      },
      "outputs": [],
      "source": [
        "def prepare_data():\n",
        "    import pandas as pd\n",
        "    print(\"---- Inside prepare_data component ----\")\n",
        "    # Load dataset\n",
        "    df = pd.read_csv(\"https://raw.githubusercontent.com/TripathiAshutosh/dataset/main/iris.csv\")\n",
        "    df = df.dropna()\n",
        "    df.to_csv(f'data/final_df.csv', index=False)\n",
        "    print(\"\\n ---- data csv is saved to PV location /data/final_df.csv ----\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "w52L0ZLHoK0E"
      },
      "outputs": [],
      "source": [
        "def train_test_split():\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    print(\"---- Inside train_test_split component ----\")\n",
        "    final_data = pd.read_csv(f'data/final_df.csv')\n",
        "    target_column = 'class'\n",
        "    X = final_data.loc[:, final_data.columns != target_column]\n",
        "    y = final_data.loc[:, final_data.columns == target_column]\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,stratify = y, random_state=47)\n",
        "\n",
        "    np.save(f'data/X_train.npy', X_train)\n",
        "    np.save(f'data/X_test.npy', X_test)\n",
        "    np.save(f'data/y_train.npy', y_train)\n",
        "    np.save(f'data/y_test.npy', y_test)\n",
        "\n",
        "    print(\"\\n---- X_train ----\")\n",
        "    print(\"\\n\")\n",
        "    print(X_train)\n",
        "\n",
        "    print(\"\\n---- X_test ----\")\n",
        "    print(\"\\n\")\n",
        "    print(X_test)\n",
        "\n",
        "    print(\"\\n---- y_train ----\")\n",
        "    print(\"\\n\")\n",
        "    print(y_train)\n",
        "\n",
        "    print(\"\\n---- y_test ----\")\n",
        "    print(\"\\n\")\n",
        "    print(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "24Z4ZqHMoK0E"
      },
      "outputs": [],
      "source": [
        "def training_basic_classifier():\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "    print(\"---- Inside training_basic_classifier component ----\")\n",
        "\n",
        "    X_train = np.load(f'data/X_train.npy',allow_pickle=True)\n",
        "    y_train = np.load(f'data/y_train.npy',allow_pickle=True)\n",
        "\n",
        "    classifier = LogisticRegression(max_iter=500)\n",
        "    classifier.fit(X_train,y_train)\n",
        "    import pickle\n",
        "    with open(f'data/model.pkl', 'wb') as f:\n",
        "        pickle.dump(classifier, f)\n",
        "\n",
        "    print(\"\\n logistic regression classifier is trained on iris data and saved to PV location /data/model.pkl ----\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "lr4nw1ayoK0F"
      },
      "outputs": [],
      "source": [
        "def predict_on_test_data():\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    import pickle\n",
        "    print(\"---- Inside predict_on_test_data component ----\")\n",
        "    with open(f'data/model.pkl','rb') as f:\n",
        "        logistic_reg_model = pickle.load(f)\n",
        "    X_test = np.load(f'data/X_test.npy',allow_pickle=True)\n",
        "    y_pred = logistic_reg_model.predict(X_test)\n",
        "    np.save(f'data/y_pred.npy', y_pred)\n",
        "\n",
        "    print(\"\\n---- Predicted classes ----\")\n",
        "    print(\"\\n\")\n",
        "    print(y_pred)\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "yzUCWTlaoK0F"
      },
      "outputs": [],
      "source": [
        "def predict_prob_on_test_data():\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    import pickle\n",
        "    print(\"---- Inside predict_prob_on_test_data component ----\")\n",
        "    with open(f'data/model.pkl','rb') as f:\n",
        "        logistic_reg_model = pickle.load(f)\n",
        "    X_test = np.load(f'data/X_test.npy',allow_pickle=True)\n",
        "    y_pred_prob = logistic_reg_model.predict_proba(X_test)\n",
        "    np.save(f'data/y_pred_prob.npy', y_pred_prob)\n",
        "\n",
        "    print(\"\\n---- Predicted Probabilities ----\")\n",
        "    print(\"\\n\")\n",
        "    print(y_pred_prob)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "NXm_yMYooK0F"
      },
      "outputs": [],
      "source": [
        "def get_metrics():\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    from sklearn.metrics import accuracy_score,precision_score,recall_score,log_loss\n",
        "    from sklearn import metrics\n",
        "    print(\"---- Inside get_metrics component ----\")\n",
        "    y_test = np.load(f'data/y_test.npy',allow_pickle=True)\n",
        "    y_pred = np.load(f'data/y_pred.npy',allow_pickle=True)\n",
        "    y_pred_prob = np.load(f'data/y_pred_prob.npy',allow_pickle=True)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    prec = precision_score(y_test, y_pred,average='micro')\n",
        "    recall = recall_score(y_test, y_pred,average='micro')\n",
        "    entropy = log_loss(y_test, y_pred_prob)\n",
        "\n",
        "    y_test = np.load(f'data/y_test.npy',allow_pickle=True)\n",
        "    y_pred = np.load(f'data/y_pred.npy',allow_pickle=True)\n",
        "    print(metrics.classification_report(y_test, y_pred))\n",
        "\n",
        "    print(\"\\n Model Metrics:\", {'accuracy': round(acc, 2), 'precision': round(prec, 2), 'recall': round(recall, 2), 'entropy': round(entropy, 2)})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XD_zo67FoK0G"
      },
      "source": [
        "### Kubeflow pipeline creation work start from here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "id": "_XA-nbTdoK0G",
        "outputId": "8af4ec95-ef8e-4dbc-bfb2-1a8ec385451e"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'kfp.v2.components' has no attribute 'func_to_container_op'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-9e00e3de3e66>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m create_step_prepare_data = kfp.v2.components.func_to_container_op (\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepare_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mbase_image\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'python:3.7'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mpackages_to_install\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pandas==1.2.4'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'numpy==1.21.0'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m )\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'kfp.v2.components' has no attribute 'func_to_container_op'"
          ]
        }
      ],
      "source": [
        "create_step_prepare_data = kfp.v2.components.func_to_container_op (\n",
        "    func=prepare_data,\n",
        "    base_image='python:3.7',\n",
        "    packages_to_install=['pandas==1.2.4','numpy==1.21.0']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TGnMmemhoK0G"
      },
      "outputs": [],
      "source": [
        "create_step_train_test_split = kfp.components.create_component_from_func(\n",
        "    func=train_test_split,\n",
        "    base_image='python:3.7',\n",
        "    packages_to_install=['pandas==1.2.4','numpy==1.21.0','scikit-learn==0.24.2']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aRU2kpDOoK0G"
      },
      "outputs": [],
      "source": [
        "create_step_training_basic_classifier = kfp.components.create_component_from_func(\n",
        "    func=training_basic_classifier,\n",
        "    base_image='python:3.7',\n",
        "    packages_to_install=['pandas==1.2.4','numpy==1.21.0','scikit-learn==0.24.2']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vb4CNdW8oK0G"
      },
      "outputs": [],
      "source": [
        "create_step_predict_on_test_data = kfp.components.create_component_from_func(\n",
        "    func=predict_on_test_data,\n",
        "    base_image='python:3.7',\n",
        "    packages_to_install=['pandas==1.2.4','numpy==1.21.0','scikit-learn==0.24.2']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PHynZtuaoK0H"
      },
      "outputs": [],
      "source": [
        "create_step_predict_prob_on_test_data = kfp.components.create_component_from_func(\n",
        "    func=predict_prob_on_test_data,\n",
        "    base_image='python:3.7',\n",
        "    packages_to_install=['pandas==1.2.4','numpy==1.21.0','scikit-learn==0.24.2']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bo66pYqVoK0H"
      },
      "outputs": [],
      "source": [
        "create_step_get_metrics = kfp.components.create_component_from_func(\n",
        "    func=get_metrics,\n",
        "    base_image='python:3.7',\n",
        "    packages_to_install=['pandas==1.2.4','numpy==1.21.0','scikit-learn==0.24.2']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TK6JtZDYoK0H"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Define the pipeline\n",
        "@dsl.pipeline(\n",
        "   name='IRIS classifier Kubeflow Demo Pipeline',\n",
        "   description='A sample pipeline that performs IRIS classifier task'\n",
        ")\n",
        "# Define parameters to be fed into pipeline\n",
        "def iris_classifier_pipeline(data_path: str):\n",
        "    vop = dsl.VolumeOp(\n",
        "    name=\"t-vol\",\n",
        "    resource_name=\"t-vol\",\n",
        "    size=\"1Gi\",\n",
        "    modes=dsl.VOLUME_MODE_RWO)\n",
        "\n",
        "    prepare_data_task = create_step_prepare_data().add_pvolumes({data_path: vop.volume})\n",
        "    train_test_split = create_step_train_test_split().add_pvolumes({data_path: vop.volume}).after(prepare_data_task)\n",
        "    classifier_training = create_step_training_basic_classifier().add_pvolumes({data_path: vop.volume}).after(train_test_split)\n",
        "    log_predicted_class = create_step_predict_on_test_data().add_pvolumes({data_path: vop.volume}).after(classifier_training)\n",
        "    log_predicted_probabilities = create_step_predict_prob_on_test_data().add_pvolumes({data_path: vop.volume}).after(log_predicted_class)\n",
        "    log_metrics_task = create_step_get_metrics().add_pvolumes({data_path: vop.volume}).after(log_predicted_probabilities)\n",
        "\n",
        "\n",
        "    prepare_data_task.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
        "    train_test_split.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
        "    classifier_training.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
        "    log_predicted_class.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
        "    log_predicted_probabilities.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
        "    log_metrics_task.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
        "\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bGrDJ-k9oK0H"
      },
      "outputs": [],
      "source": [
        "\n",
        "kfp.compiler.Compiler().compile(\n",
        "    pipeline_func=iris_classifier_pipeline,\n",
        "    package_path='IRIS_Classifier_pipeline1.yaml')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E8gAY1FGoK0H"
      },
      "outputs": [],
      "source": [
        "client = kfp.Client()\n",
        "#session_cookie = \"MTY2MDY0Mjg0OXxOd3dBTkRSVE5FeElTMEZDVDFVeU5EZE1SMHhUVHpRMU5FcFpNMWRNVWpaTFVrOHlXRFJOVlRReVVFNUxOazFZVEVWQ05FUkZTRUU9fM7IcyOyK49OM0dMDjRJR85gqDksj-YOOLsagNs-_-KR\"\n",
        "# HOST = \"http://localhost:8080/\"\n",
        "# namespace = \"kubeflow\"\n",
        "# client = kfp.Client(\n",
        "#     host=f\"{HOST}/pipeline\",\n",
        "#     #cookies=f\"authservice_session={session_cookie}\",\n",
        "#     namespace=namespace,\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-uWSysypoK0H",
        "outputId": "70a1f9b2-1355-48bc-ac2d-d2cf37678975"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-12-24\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<a href=\"/pipeline/#/experiments/details/b646408d-38a6-44c5-afa2-7052ddd42e5f\" target=\"_blank\" >Experiment details</a>."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<a href=\"/pipeline/#/runs/details/a5aa8675-76cc-4888-9582-1fed5517b043\" target=\"_blank\" >Run details</a>."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "DATA_PATH = '/data'\n",
        "\n",
        "import datetime\n",
        "print(datetime.datetime.now().date())\n",
        "\n",
        "\n",
        "pipeline_func = iris_classifier_pipeline\n",
        "experiment_name = 'iris_classifier_exp' +\"_\"+ str(datetime.datetime.now().date())\n",
        "run_name = pipeline_func.__name__ + ' run'\n",
        "namespace = \"kubeflow\"\n",
        "\n",
        "arguments = {\"data_path\":DATA_PATH}\n",
        "\n",
        "kfp.compiler.Compiler().compile(pipeline_func,\n",
        "  '{}.zip'.format(experiment_name))\n",
        "\n",
        "run_result = client.create_run_from_pipeline_func(pipeline_func,\n",
        "                                                  experiment_name=experiment_name,\n",
        "                                                  run_name=run_name,\n",
        "                                                  arguments=arguments)\n",
        "\n",
        "# from kubernetes import client as k8s_client\n",
        "# pipeline_conf = kfp.dsl.PipelineConf()\n",
        "# pipeline_conf.set_image_pull_secrets([k8s_client.V1ObjectReference(namespace='kubeflow',\n",
        "#                                                                                  name=\"secret\")])\n",
        "# pipeline_conf.set_image_pull_policy(\"IfNotPresent\")\n",
        "\n",
        "# Compile pipeline to generate compressed YAML definition of the pipeline.\n",
        "# kfp.compiler.Compiler().compile(pipeline_func,\n",
        "#   '{}.zip'.format(experiment_name))\n",
        "\n",
        "# Submit pipeline directly from pipeline function\n",
        "# run_result = client.create_run_from_pipeline_func(pipeline_func,\n",
        "#                                                   experiment_name=experiment_name,\n",
        "#                                                   run_name=run_name,\n",
        "#                                                   arguments=arguments,\n",
        "#                                                   namespace = namespace,\n",
        "#                                                   pipeline_conf=pipeline_conf)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BFkuoK5poK0I"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "seth63rfoK0I"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    },
    "vscode": {
      "interpreter": {
        "hash": "11938c6bc6919ae2720b4d5011047913343b08a43b18698fd82dedb0d4417594"
      }
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}